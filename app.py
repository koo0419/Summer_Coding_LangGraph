# FINAL_PROJECT/app.py (ìµœì¢… ì™„ì„±ë³¸)

# venv/scripts/activate
# cd mcp_server
# uvicorn main:app --reload

import gradio as gr
import uuid
import json
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
import requests
import os
import threading

from graph.builder import graph
from agents.market_agent import generate_market_briefing, send_market_briefing_email

# --- CSS íŒŒì¼ ì§ì ‘ ì½ì–´ì˜¤ê¸° ---
try:
    with open("style.css", "r", encoding="utf-8") as f:
        custom_css = f.read()
except FileNotFoundError:
    print("ê²½ê³ : style.css íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ìŠ¤íƒ€ì¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    custom_css = ""

# --- í—¬í¼ í•¨ìˆ˜ ---
def parse_tool_call(tool_calls):
    if not tool_calls:
        return ""
    call = tool_calls[0]
    return f"**ë„êµ¬:** `{call['name']}`\n**íŒŒë¼ë¯¸í„°:** `{json.dumps(call['args'], ensure_ascii=False)}`"

def synthesize_final_question(original_question: str, modification_request: str) -> str:
    """LLMì„ ì´ìš©í•´ ì›ë˜ ì§ˆë¬¸ê³¼ ìˆ˜ì • ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        llm = ChatOpenAI(model=os.getenv("TOOL_LLM_MODEL", "gpt-4o-mini"), temperature=0)
        prompt = f"""
        ê¸°ì¡´ ì§ˆë¬¸ê³¼ ì‚¬ìš©ìì˜ ìˆ˜ì • ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ, í•˜ë‚˜ì˜ ì™„ì„±ëœ ìµœì¢… ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•´ì¤˜.
        ì˜¤ì§ ì¬êµ¬ì„±ëœ ì§ˆë¬¸ ìì²´ë§Œ ê°„ê²°í•˜ê²Œ ë°˜í™˜í•´. ë‹¤ë¥¸ ë¶€ì—° ì„¤ëª…ì€ ì ˆëŒ€ ë§ë¶™ì´ì§€ ë§ˆ.
        # ê¸°ì¡´ ì§ˆë¬¸:
        {original_question}
        # ìˆ˜ì • ìš”ì²­:
        {modification_request}
        # ìµœì¢… ì§ˆë¬¸:
        """
        response = llm.invoke(prompt.strip())
        return response.content.strip()
    except Exception as e:
        print(f"âŒ ì§ˆë¬¸ ì¬êµ¬ì„± ì‹¤íŒ¨: {e}")
        return modification_request

# --- Notion ê¸°ë¡ í•¨ìˆ˜ ---
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://127.0.0.1:8000")

def record_chat_to_notion(user_input: str, ai_response: str):
    try:
        url = f"{MCP_SERVER_URL}/record_chat"
        payload = {"user_question": user_input, "ai_response": ai_response}
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        print("âœ… ëŒ€í™” ë‚´ìš©ì´ Notionì— ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except requests.exceptions.RequestException as e:
        print(f"âŒ Notion ê¸°ë¡ ì‹¤íŒ¨: {e}")

# --- ì´ë©”ì¼ ë°œì†¡ í•¨ìˆ˜ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©) ---
def send_briefing_in_background():
    """ë‰´ìŠ¤ ìš”ì•½ ì´ë©”ì¼ì„ ìƒì„±í•˜ê³  ë°œì†¡í•©ë‹ˆë‹¤."""
    print("ğŸš€ ì•± ì‹œì‘ ì‹œ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì´ë©”ì¼ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°œì†¡í•©ë‹ˆë‹¤...")
    try:
        summary = generate_market_briefing.invoke({})
        result = send_market_briefing_email(summary, "jeasungkoo@gmail.com")
        print("ğŸ“¬ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ!", result)
    except Exception as e:
        print(f"âŒ ì´ë©”ì¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# --- Gradio ì•± ë¡œì§ ---
with gr.Blocks(theme=gr.themes.Base(), css=custom_css) as demo:
    thread_id = gr.State(value="")
    tool_call_info = gr.State(value={})
    active_question = gr.State(value="")

    gr.Markdown(
        """
        # ğŸ¤– ì£¼ì‹ ì¡°ì–¸ AI ì‹œìŠ¤í…œ (LangGraph ver.)
        ê¶ê¸ˆí•œ ì£¼ì‹ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”! AIê°€ ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ë•Œ, ì‹¤í–‰ ê¶Œí•œì„ ë¬¼ì–´ë³¼ ê±°ì˜ˆìš”.
        """
    )

    with gr.Row(equal_height=True, elem_id="two_col"):
        # ì™¼ìª½: ì±„íŒ… + ì…ë ¥ì°½
        with gr.Column(scale=7, min_width=600):
            chatbot = gr.Chatbot(
                [],
                elem_id="chatbot",
                type="messages",
                height=600,
            )

            with gr.Row():
                txt = gr.Textbox(
                    scale=4,
                    show_label=False,
                    placeholder="ê¶ê¸ˆí•œ ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”.",
                    container=False,
                )

        # ì˜¤ë¥¸ìª½: ë„êµ¬ ìŠ¹ì¸/ìˆ˜ì • íŒ¨ë„
        with gr.Column(scale=3, min_width=320, elem_id="hil_panel", visible=False) as hil_section:
            gr.Markdown("**AIê°€ ë‹¤ìŒ ì‘ì—…ì„ ê³„íší–ˆìŠµë‹ˆë‹¤. ì§„í–‰í• ê¹Œìš”?**")
            tool_plan = gr.Markdown()

            with gr.Row():
                with gr.Column(scale=1, min_width=120):
                    approve_btn = gr.Button("âœ… ì§„í–‰", variant="primary")
                with gr.Column(scale=2):
                    modify_input = gr.Textbox(show_label=False, placeholder="ìˆ˜ì •í•  ë‚´ìš© ì…ë ¥...")
                    modify_btn = gr.Button("ğŸ“ íŒŒë¼ë¯¸í„° ìˆ˜ì •")
                with gr.Column(scale=2):
                    reject_input = gr.Textbox(show_label=False, placeholder="ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥...")
                    reject_btn = gr.Button("âŒ ìƒˆ ì§ˆë¬¸ ì…ë ¥")

    def handle_user_message(user_message, history, tid):
        if not user_message.strip():
            return history, tid, "", {}, gr.update(visible=False), gr.update(value="", interactive=True), user_message
            
        if not tid:
            tid = str(uuid.uuid4())
        config = {"configurable": {"thread_id": tid}}
        
        history.append({"role": "user", "content": user_message})
        
        input_message = HumanMessage(content=user_message)
        
        last_event = None
        for event in graph.stream({"messages": [input_message]}, config=config, stream_mode="values"):
            last_event = event

        messages = last_event.get('messages', [])
        if not messages:
            history.append({"role": "assistant", "content": "ì˜¤ë¥˜: AI ì‘ë‹µ ì²˜ë¦¬ ë¶ˆê°€"})
            return history, tid, "", {}, gr.update(visible=False), gr.update(value="", interactive=True), user_message

        last_ai_message = messages[-1]

        if not hasattr(last_ai_message, 'tool_calls') or not last_ai_message.tool_calls:
            history.append({"role": "assistant", "content": last_ai_message.content})
            record_chat_to_notion(user_message, last_ai_message.content)
            return history, tid, "", {}, gr.update(visible=False), gr.update(value="", interactive=True), user_message

        tool_calls = last_ai_message.tool_calls
        history.append({"role": "assistant", "content": "ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•©ë‹ˆë‹¤..."})
        return (
            history, tid, parse_tool_call(tool_calls),
            tool_calls, gr.update(visible=True), gr.update(value="", interactive=False), user_message
        )

    def handle_hil_decision(decision, new_input, history, tid, original_tool_calls, current_question):
        config = {"configurable": {"thread_id": tid}}
        
        question_to_log = current_question
        stream_input = None

        if decision == "modify" or decision == "reject":
            # â­ï¸ í•µì‹¬ ìˆ˜ì • 1: ì‚¬ìš©ìì˜ ìƒˆ ì§ˆë¬¸ì„ ì±„íŒ… ê¸°ë¡ì— ë¨¼ì € ì¶”ê°€í•©ë‹ˆë‹¤.
            history.append({"role": "user", "content": new_input})
            
            tool_call_id = original_tool_calls[0]['id']
            feedback_tool_message = ToolMessage(
                content=f"ì‚¬ìš©ìê°€ ì´ì „ ê³„íšì„ ê±°ì ˆí•˜ê³  ìƒˆë¡œìš´ ì§€ì‹œë¥¼ ë‚´ë ¸ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ì´ì „ ê³„íšì€ ë¬´ì‹œí•˜ê³ , ì´ ìƒˆë¡œìš´ ì§€ì‹œë¥¼ ë”°ë¼ì£¼ì„¸ìš”: '{new_input}'",
                tool_call_id=tool_call_id
            )
            stream_input = {"messages": [feedback_tool_message, HumanMessage(content=new_input)]}
            question_to_log = synthesize_final_question(current_question, new_input)
        else: # approve
            stream_input = None
        
        # â­ï¸ í•µì‹¬ ìˆ˜ì • 2: AIì˜ ë‹µë³€ì„ ìœ„í•œ ë¹ˆ ê³µê°„(placeholder)ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        history.append({"role": "assistant", "content": "ìš”ì²­ ì²˜ë¦¬ ì¤‘..."})
        
        last_event = None
        for event in graph.stream(stream_input, config=config, stream_mode="values"):
            last_event = event

        messages = last_event.get('messages', [])
        if not messages:
            history[-1] = {"role": "assistant", "content": "ì˜¤ë¥˜: AI ì‘ë‹µ ì²˜ë¦¬ ë¶ˆê°€"}
            return history, tid, "", {}, gr.update(visible=False), gr.update(interactive=True), question_to_log

        last_ai_message = messages[-1]
        
        if not hasattr(last_ai_message, 'tool_calls') or not last_ai_message.tool_calls:
            final_answer = last_ai_message.content
            history[-1] = {"role": "assistant", "content": final_answer}
            record_chat_to_notion(question_to_log, final_answer)
            return history, tid, "", {}, gr.update(visible=False), gr.update(interactive=True), question_to_log
        else:
            tool_calls = last_ai_message.tool_calls
            history[-1] = {"role": "assistant", "content": "ê³„íšì„ ìˆ˜ì •í•˜ì—¬ ë‹¤ì‹œ ì œì•ˆí•©ë‹ˆë‹¤..."}
            return (
                history, tid, parse_tool_call(tool_calls),
                tool_calls, gr.update(visible=True), gr.update(interactive=False), question_to_log
            )

    # ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ
    txt.submit(
        handle_user_message,
        [txt, chatbot, thread_id],
        [chatbot, thread_id, tool_plan, tool_call_info, hil_section, txt, active_question]
    )
    
    approve_btn.click(
        handle_hil_decision,
        [gr.State("approve"), gr.State(None), chatbot, thread_id, tool_call_info, active_question],
        [chatbot, thread_id, tool_plan, tool_call_info, hil_section, txt, active_question]
    )
    modify_btn.click(
        handle_hil_decision,
        [gr.State("modify"), modify_input, chatbot, thread_id, tool_call_info, active_question],
        [chatbot, thread_id, tool_plan, tool_call_info, hil_section, txt, active_question]
    )
    reject_btn.click(
        handle_hil_decision,
        [gr.State("reject"), reject_input, chatbot, thread_id, tool_call_info, active_question],
        [chatbot, thread_id, tool_plan, tool_call_info, hil_section, txt, active_question]
    )

if __name__ == "__main__":
    email_thread = threading.Thread(target=send_briefing_in_background)
    email_thread.daemon = True
    email_thread.start()

    demo.launch()