# FINAL_PROJECT/tools/term_explain_tool.py

# finance_terms.pdf íŒŒì¼ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ìê°€ ëª¨ë¥¼ ìˆ˜ ìˆëŠ” ì „ë¬¸ ê¸ˆìœµ ìš©ì–´ë¥¼ ì •í™•í•˜ê²Œ ì„¤ëª…í•˜ëŠ” RAG íŒŒì´í”„ë¼ì¸
import os
from typing import List, Tuple

from dotenv import load_dotenv
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# ===== ê¸°ë³¸ ì„¤ì • =====
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

PDF_PATH = os.path.join("data", "finance_terms.pdf")
PERSIST_DIR = os.path.join("data", "chroma_terms")

# í•œê¸€ ì˜ ë˜ëŠ” ìµœì‹  ì„ë² ë”© ì§€ì • (í•˜ë‚˜ë§Œ ë§Œë“¤ì–´ ì¬ì‚¬ìš©)
EMB = OpenAIEmbeddings(model="text-embedding-3-small", api_key=OPENAI_API_KEY)

# ìºì‹œ(ìµœì´ˆ 1íšŒ ë¡œë“œ)
_DOC_CHUNKS = None
_BM25 = None


# PDF ë¬¸ì„œë¥¼ ì˜ê²Œ ìª¼ê°œ(Chunking) ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ChromaDBë¼ëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
def build_vectorstore():
    """PDF â†’ ì²­í¬ â†’ ì„ë² ë”©(ì†Œë°°ì¹˜) â†’ Chroma (ìë™ ì €ì¥)"""
    print("ğŸ“„ PDF ë¡œë”© ì¤‘...")
    loader = PyMuPDFLoader(PDF_PATH)
    docs = loader.load()
    print(f"âœ… {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_documents(docs)
    print(f"âœ‚ï¸ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

    # persist_directory ì§€ì •ë§Œ í•˜ë©´ ìë™ ì €ì¥ë¨ (persist() í˜¸ì¶œ X)
    vs = Chroma(embedding_function=EMB, persist_directory=PERSIST_DIR)

    BATCH = 64
    for i in range(0, len(chunks), BATCH):
        batch = chunks[i:i + BATCH]
        print(f"â¡ï¸ ì¸ë±ì‹± ì¤‘... {i+1} ~ {i+len(batch)}")
        vs.add_documents(batch)

    print("âœ… ì¸ë±ì‹± ì™„ë£Œ (ìë™ ì €ì¥ë¨):", PERSIST_DIR)

def _load_vectorstore() -> Chroma:
    """persistëœ Chroma ë¶ˆëŸ¬ì˜¤ê¸°"""
    return Chroma(embedding_function=EMB, persist_directory=PERSIST_DIR)


def _load_chunks_for_bm25() -> Tuple[List, BM25Retriever]:
    """BM25ìš© ì²­í¬/ë¦¬íŠ¸ë¦¬ë²„ ë©”ëª¨ë¦¬ ë¡œë“œ(ìµœì´ˆ 1íšŒ)"""
    global _DOC_CHUNKS, _BM25
    if _DOC_CHUNKS is None:
        loader = PyMuPDFLoader(PDF_PATH)
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=120)
        _DOC_CHUNKS = splitter.split_documents(docs)
        _BM25 = BM25Retriever.from_documents(_DOC_CHUNKS)
        _BM25.k = 6
    return _DOC_CHUNKS, _BM25


def _format_sources(docs: List) -> str:
    """ì¶œì²˜ í˜ì´ì§€ í‘œì‹œ (1-base)"""
    pages = sorted({(d.metadata.get("page", 0) + 1) for d in docs})
    if not pages:
        return ""
    page_str = ", ".join(map(str, pages[:8])) + ("â€¦" if len(pages) > 8 else "")
    return f"\n\nì¶œì²˜: í•œêµ­ì€í–‰ ã€ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ ã€ (p. {page_str})"


# ===== RAG ë©”ì¸ =====
def explain_term(query: str, k: int = 4) -> str:
    """PDF ê¸°ë°˜ RAG: ì •ì˜ â†’ í•µì‹¬ í¬ì¸íŠ¸ â†’ í•œ ì¤„ ì˜ˆì‹œ (+ì¶œì²˜ í˜ì´ì§€)"""
    if not os.path.exists(PERSIST_DIR) or not os.listdir(PERSIST_DIR):
        return "ì•„ì§ ìš©ì–´ì§‘ ì¸ë±ìŠ¤ê°€ ì—†ì–´ìš”. ë¨¼ì € ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."

    vs = _load_vectorstore()

    # 1) ì„ë² ë”© ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒˆ API: invoke)
    retriever = vs.as_retriever(search_kwargs={"k": max(8, k)})
    try:
        contexts = retriever.invoke(query)
    except Exception:
        # êµ¬ë²„ì „ í˜¸í™˜
        contexts = retriever.get_relevant_documents(query)

    # 2) ë¹ˆì•½í•˜ë©´ BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ë³‘í•©
    if len(contexts) < 2:
        _, bm25 = _load_chunks_for_bm25()
        bm_hits = bm25.get_relevant_documents(query)
        seen, merged = set(), []
        for d in (contexts + bm_hits):
            key = (d.metadata.get("page"), d.page_content[:60])
            if key not in seen:
                merged.append(d)
                seen.add(key)
        contexts = merged[:max(8, k)]

    if not contexts:
        return f"'{query}' ê´€ë ¨ ë‚´ìš©ì„ PDFì—ì„œ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë¬¼ì–´ë´ì¤„ë˜?"

    # ì»¨í…ìŠ¤íŠ¸ ë¬¶ê¸°
    ctx_texts = "\n\n".join([c.page_content.strip() for c in contexts])

    # LLM í˜¸ì¶œ
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=OPENAI_API_KEY)
    system_msg = (
        "ë„ˆëŠ” ê¸ˆìœµ ìš©ì–´ ì„¤ëª… ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì—ë§Œ ê·¼ê±°í•´ ë‹µí•´.\n"
        "í˜•ì‹: â‘  ì •ì˜(ë‘ì„¸ ë¬¸ì¥) â‘¡ í•µì‹¬ í¬ì¸íŠ¸(ë¶ˆë¦¿ 3~5ê°œ) â‘¢ í•œ ì¤„ ì˜ˆì‹œ\n"
        "ì¶œì²˜ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´."
    )
    user_msg = (
        f"[ì§ˆë¬¸]\n{query}\n\n"
        f"[ì»¨í…ìŠ¤íŠ¸]\n{ctx_texts}\n\n"
        "ìœ„ ì»¨í…ìŠ¤íŠ¸ë§Œ í™œìš©í•´ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜."
    )
    resp = llm.invoke([
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg},
    ])
    answer = resp.content.strip()
    return answer + _format_sources(contexts)


# === LangChain Tool ë˜í¼ ===
from langchain.tools import Tool

def get_term_explain_tool() -> Tool:
    """
    LangChain Toolë¡œ ê°ì‹¸ì„œ ZeroShotAgent/Routerì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•˜ê²Œ í•¨.
    """
    return Tool(
        name="TermExplain",
        description=(
            "í•œêµ­ì€í–‰ ã€ê²½ì œê¸ˆìœµìš©ì–´ 700ì„ ã€ PDF ê¸°ë°˜ìœ¼ë¡œ ê²½ì œ/ê¸ˆìœµ ìš©ì–´ë¥¼ ì„¤ëª…í•œë‹¤. "
            "ì‚¬ìš©ìëŠ” ìš©ì–´(ì˜ˆ: ë””í”Œë ˆì´ì…˜, ë“€ë ˆì´ì…˜, í…Œì´í¼ë§ ë“±)ë¥¼ í•œêµ­ì–´ë¡œ ë¬¼ì–´ë³¸ë‹¤."
        ),
        func=lambda q: explain_term(q),
    )